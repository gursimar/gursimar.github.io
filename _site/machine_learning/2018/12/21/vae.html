<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Gursimran - Generative models</title>
  <meta name="author" content="Gursimran" />
  <meta name="description" content="The blog of Gursimran" />
  <link rel="canonical" href="http://localhost:4000/machine_learning/2018/12/21/vae.html" />

  <script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  
  <link href="//fonts.googleapis.com/css?family=Open+Sans:600,800" rel="stylesheet" type="text/css">
  <link rel="shortcut icon" href="/favicon.png">
  <link rel="alternate" type="application/atom+xml" title="Gursimran" href="http://localhost:4000/atom.xml" />

  <link rel="stylesheet" href="/assets/css/all.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha256-k2/8zcNbxVIh5mnQ52A0r3a6jAgMGxFJFE2707UxGCk= sha512-ZV9KawG2Legkwp3nAlxLIVFudTauWuBpC10uEafMHYL0Sarrz5A7G79kXh5+5+woxQ5HM559XX2UZjMJ36Wplg==" crossorigin="anonymous">
</head>
<body>
  <div class="container">
    <div class="four columns sidebar">
      <!-- _includes/base.html -->



<nav>
  <div style="float:left">
  <h1 style='margin-left:-25%'>Gursimran</h1>
  <img style='height: 75%; width: 75%; border-radius: 50%; margin-left:-1%; object-fit: contain' class="img-circle avatar" src="/assets/pic.png" id="logo" alt="Blog logo"/>
  </div>

  <div id="bio">
      <br>
    <!--p><code>machine leanring<br>education at scale<br>DIY electronics.</code></p>-->
  </div>

  <div id="links">
    <a href="/index">home</a></br>
    <a href="/projects">projects</a></br>
    <a href="/blog">blog</a></br>
    <a href="/cv.pdf">cv - updated</a></br>


  </div>
  
  </br></br>
  
  <div id="social">
    Follow me:
<div id="stalker">
  
  <a title="gursimar on Github" href="https://github.com/gursimar">
    <i class="fa fa-github-square"></i>
  </a>
  

  

  

  

  

  

  
  <a title="Gursimran on Facebook" href="https://facebook.com/singh.gursimr">
    <i class="fa fa-facebook-square"></i>
  </a>
  

  

  
  <a title="Gursimran on LinkedIn" href="https://www.linkedin.com/in/gursimar">
    <i class="fa fa-linkedin-square"></i>
  </a>
  

  
  <a title="Gursimran on Google Plus" href="https://plus.google.com/+GursimransinghMuhar">
    <i class="fa fa-google-plus-square"></i>
  </a>
  

  

  
  <a title="Atom feed" id="atom" href="/atom.xml">
    <i class="fa fa-rss-square"></i>
  </a>
  
</div>

  </div>
</nav>

    </div>

    <div class="eleven columns content">
      <p class="meta">
  December 21, 2018 
  <a href="/">
    <i class="home fa fa-home"></i>
  </a>
</p>

<h1 class="title">Generative models</h1>

<div id="post">
  <p class="text-justify">Machine learning algorithms can be classified into two broad categories. Discriminative models learn the conditional probability distribution of labels given the data p(y|x). These models are easy to train, however, they lack the ability to understand the underlying distribution of the data x. Generative models, in contrast, learn the joint probability density p(x,y). The ability to jointly model data $x$ and labels $y$, allows them to generalize better in case of limited and/ or missing data. They can also be used to compute conditional density using the Bayes rule <script type="math/tex">p(y\|x)</script>.  \cite{ng2002discriminative} provides a nice analysis of discriminative vs generative models and the situations in which we prefer one over the other. Additionally, they can also be used to generate new samples $(x,y)$ from the distribution. For instance, generative models are used to generate new hypothetical faces of celebrity, generate more text in a particular handwriting style and new artificial environments to be used in reinforcement learning. Other applications include super-resolution \cite{ledig2016photo}, conversion of sketches to images \cite{chen2018sketchygan}, etc. <a href="#ruby">(Flanagan &amp; Matsumoto, 2008)</a></p>

<p class="text-justify">Typically, we train generative models by maximizing the likelihood of the training data <script type="math/tex">\theta^* = arg \max_{\theta} \sum_{i=1}^{N} log(p_{model}(x_i;\theta))</script>. However, optimizing the maximum likelihood objective often becomes intractable in the general formulation of <script type="math/tex">p_{model}</script>. A wide range of models like Gaussian mixture models, Markov chains, and hidden Markov models makes simplifying conditional independence assumptions to make the objective tractable. However, due to the simplifying assumptions, these models are limited in terms of expressiveness. Hence they are not able to model many data distributions of interest which are often non-linear and highly complex. Deep generative models relax some of these strong assumptions and model the joint density using neural networks as a powerful function approximator. Deep generative models can be further classified as explicit density models and implicit density models based on how we define likelihood, its gradients, and any approximations.</p>

<p class="text-justify">Explicit density models define an explicit function <script type="math/tex">p_{model}(x;\theta)</script> to represent the probability density of the data. Explicit models that use approximations for the intractable likelihood allow us to use a large class of powerful models. Instead of optimizing the intractable likelihood directly, these models optimize a lower bound approximation of the objective. VAEs fall in this category. The main disadvantage of these methods is that when we have a weak approximation of the posterior or the prior distribution, optimizing the lower bound does not effectively learn the target distribution. Implicit density models do not model the probability density explicitly. Instead, it just provides a stochastic procedure to generate data. GANs fall under this category. These models are trained by directly sampling from the <script type="math/tex">p_{model}</script> using a game theoretic approach between two networks, the generator and the discriminator. However, finding an equilibrium is hard and it makes them hard to train.</p>

<h2 id="references">References</h2>
<ol class="bibliography"><li><span id="ruby">Flanagan, D., &amp; Matsumoto, Y. (2008). <i>The Ruby Programming Language</i>. O’Reilly Media.</span></li></ol>

</div>

<div id="related">
  <h3>Related Posts</h3>
  <ul class="posts">
    
    <li>
      <span>17 Jul 2017 &raquo;</span> <a href="/travel/2017/07/17/Prashar-Lake.html">Weekend excursion to Prashar lake</a>
    </li>
    
  </ul>
</div>


      <div class="footer">
        <div class="disclaimer">
  
  <!--p>
    The postings on this site are my own and don't necessarily represent my 
    employer’s positions, strategies or opinions.
  </p-->
  

  <p>
    © Gursimran, 2014 &mdash; built with <a href="http://jekyllrb.com/">Jekyll</a> using <a href="https://github.com/swanson/lagom">Lagom theme</a>
  </p>
</div>
      </div>
    </div>
  </div>


<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-xxxx-x']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

</body>
</html>
